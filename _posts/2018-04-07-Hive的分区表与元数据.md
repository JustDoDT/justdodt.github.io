---
layout:     post
title:      "Hive的分区表与元数据."
date:       2018-04-07 23:01:00
author:     "JustDoDT"
header-img: "img/haha.jpg"
catalog: true
tags:
    - hadoop, hive
---


### 分区表

    /user/hive/warehouse/emp/d=20180808/.....
	  /user/hive/warehouse/emp/d=20180809/.....
  	select .... from table where d='20180808'
	
**大数据的瓶颈：IO,disk,network**
 	
#### 创建分区表
    create table order_partition(
    order_no string,
    event_time string
    )
    PARTITIONED BY(event_month string)
    ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

#### 加载数据
    load data local inpath '/home/hadoop/data/order.txt' overwrite into table order_partition
    PARTITION  (event_month='2014-05');

**再创建一个分区**

    hdfs dfs -mkdir /user/hive/warehouse/d6_hive.db/order_partition/event_month=2014-06

**把Linux本地的数据上传到HDFS新建的目录下**

    hdfs dfs -put /home/hadoop/data/order.txt /user/hive/warehouse/d6_hive.db/order_partition/event_month=2014-06
 
**然而在hive中没有出现分区6 的数据，在mysql中的partiion中也没有出现；需要对分区表做如下的修改才行的**

    mysql> select * from partitions   ;
    +---------+-------------+------------------+---------------------+-------+--------+
    | PART_ID | CREATE_TIME | LAST_ACCESS_TIME | PART_NAME           | SD_ID | TBL_ID |
    +---------+-------------+------------------+---------------------+-------+--------+
    |       6 |  1553373298 |                0 | event_month=2014-05 |    47 |     46 |
    +---------+-------------+------------------+---------------------+-------+--------+
    1 row in set (0.00 sec)

**在hive中，修改分区表**

    ALTER TABLE order_partition ADD IF NOT EXISTS PARTITION (event_month='2014-06') ;

**在MySQL中查看元数据信息**

    mysql> select * from partitions   ;
    +---------+-------------+------------------+---------------------+-------+--------+
    | PART_ID | CREATE_TIME | LAST_ACCESS_TIME | PART_NAME           | SD_ID | TBL_ID |
    +---------+-------------+------------------+---------------------+-------+--------+
    |       6 |  1553373298 |                0 | event_month=2014-05 |    47 |     46 |
    |       7 |  1553373768 |                0 | event_month=2014-06 |    48 |     46 |
    +---------+-------------+------------------+---------------------+-------+--------+
    2 rows in set (0.00 sec)

**在hive中查询分区信息**

    hive (d6_hive)> show partitions order_partition;
    OK
    event_month=2014-05
    event_month=2014-06
    Time taken: 0.06 seconds, Fetched: 2 row(s)
    hive (d6_hive)> 


#### 创建多级分区表
    create table order_mulit_partition(
    order_no string,
    event_time string
    )
    PARTITIONED BY(event_month string, step string)
    ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
    
    load data local inpath '/home/hadoop/data/order.txt' overwrite into table order_mulit_partition
    PARTITION  (event_month='2014-05', step='1');
    
#### 在Hive中查询结果 
    hive (d6_hive)> select * from order_mulit_partition where event_month='2014-05' and step='1';
    OK
    10703007267488   2014-05-10 06:01:12.334+01     2014-05 1
    10101043509689   2014-05-10 07:36:23.342+01     2014-05 1
    10101043529689   2014-05-10 07:45:23.32+01      2014-05 1
    10101043549689   2014-05-10 07:51:23.32+01      2014-05 1
    10101043539689   2014-05-10 07:57:23.342+01     2014-05 1
            NULL    2014-05 1
    Time taken: 0.223 seconds, Fetched: 6 row(s)
    hive (d6_hive)> 


**按照部门编号写到指定的分区中去 **

#### 静态分区

    CREATE TABLE `emp_static_partition`(
      `empno` int, 
      `ename` string, 
      `job` string, 
      `mgr` int, 
      `hiredate` string, 
      `sal` double, 
      `comm` double)
    partitioned by(deptno int)  
    ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
    	
    insert into table emp_static_partition PARTITION (deptno=10) 
    select empno,ename,job,mgr,hiredate,sal,comm from emp 
    where deptno=10;


    select * from emp_static_partition where deptno=10; --会执行MapReduce程序


#### 动态分区	

    CREATE TABLE `emp_dynamic_partition`(
      `empno` int, 
      `ename` string, 
      `job` string, 
      `mgr` int, 
      `hiredate` string, 
      `sal` double, 
      `comm` double)
    partitioned by(deptno int)  
    ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
    
    
    hive (d6_hive)> insert into table emp_dynamic_partition partition (deptno)
                  > select empno,ename,job,mgr,hiredate,sal,comm,deptno from emp;
    FAILED: SemanticException [Error 10096]: Dynamic partition strict mode requires at least one static partition column. To turn this off set hive.exec.dynamic.partition.mode=nonstrict

**注意：这是为严格模式，需要修改模式**

    hive (d6_hive)> 
    hive (d6_hive)>set hive.exec.dynamic.partition.mode=nonstrict；
    hive (d6_hive)>insert into table emp_dynamic_partition PARTITION (deptno) select empno,ename,job,mgr,hiredate,sal,comm,deptno from emp;


#### 静态分区和动态分区对比
静态分区，不够智能，需要认为指定分区号；动态分区更加简单，不需要认为的去指定分区号，只需要指定分区的字段就行。
	

#### hiveserver2+beeline配合使用操作Hive

Server + Client

thriftserver+beeline

操作语句：

    beeline -u jdbc:hive2://hadoop000:10000/d6_hive -n hadoop	
	
	
### 复杂数据类型： 
复杂的数据类型，需要考虑如何存，如何取的问题，特别是如何取数据的问题。

#### array类型

    create table hive_array(
    name string,
    work_locations array<string>
    )
    ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
    COLLECTION ITEMS TERMINATED BY ',';
    	
    load data local inpath '/home/hadoop/data/hive_array.txt' 
    overwrite into table hive_array;

**查询出work_locations中的数组其中的元素**	

    hive (d6_hive)> select name,work_locations[0] from hive_array;
    OK
    justdodt        beijjing
    hahh    chengdu	
	
**查询work_locations 的个数**

    hive (d6_hive)> select size(work_locations) from hive_array;
    OK
    4
    4

	
**查询在深圳上班的人**	

    hive (d6_hive)> select * from hive_array where array_contains(work_locations,'shenzhen');
    OK
    justdodt        ["beijjing","shanghai","guangzhou","shenzhen"]


#### map 类型

map : key-value

father:xiaoming#mother:xiaohuang#brother:xiaoxu

**创建表**

    create table hive_map(
    id int,
    name string,
    members map<string,string>,
    age int
    )
    ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
    COLLECTION ITEMS TERMINATED BY '#'
    MAP KEYS TERMINATED BY ':';
    	
    load data local inpath '/home/hadoop/data/hive_map.txt' 
    overwrite into table hive_map;

**查询家庭成员的母亲**

    hive (d6_hive)> select id,name,age,members['mother'] from hive_map;
    OK
    1       zhangsan        28      xiaohuang
    2       lisi    22      huangyi
    3       wangwu  29      ruhua
    4       mayun   26      angelababy

**求出家庭成员的亲属关系**

    hive (d6_hive)> select map_keys(members) from hive_map;
    OK
    ["father","mother","brother"]
    ["father","mother","brother"]
    ["father","mother","sister"]
    ["father","mother"]

**取值**

    hive (d6_hive)> select map_values(members) from hive_map;
    OK
    ["xiaoming","xiaohuang","xiaoxu"]
    ["mayun","huangyi","guanyu"]
    ["wangjianlin","ruhua","jingtian"]
    ["mayongzhen","angelababy"]

**求出亲属关系的个数**

    hive (d6_hive)> select size(members) from hive_map;
    OK
    3
    3
    3
    2


#### 结构体类型
struct   struct('a',1,2,3,4)

创建表，存储结构体

    create table hive_struct(
    ip string,
    userinfo struct<name:string,age:int>
    )
    ROW FORMAT DELIMITED FIELDS TERMINATED BY '#'
    COLLECTION ITEMS TERMINATED BY ':';
    	
    load data local inpath '/home/hadoop/data/hive_struct.txt' 
    overwrite into table hive_struct;

**查询结构体后面的用户的姓名和年龄**

    hive (d6_hive)> select userinfo.name,userinfo.age from hive_struct;
    OK
    zhangsan        40
    lisi    50
    wangwu  60
    zhaoliu 70

### Hive元数据
在hive-site.xml配置中，把hive 的元数据存储在MySQL中

    mysql> show tables;
    +---------------------------+
    | Tables_in_ruoze_d6        |
    +---------------------------+
    | bucketing_cols            |
    | cds                       |
    | columns_v2                |
    | database_params           |
    | dbs                       |
    | func_ru                   |
    | funcs                     |
    | global_privs              |
    | idxs                      |
    | index_params              |
    | part_col_privs            |
    | part_col_stats            |
    | part_privs                |
    | partition_key_vals        |
    | partition_keys            |
    | partition_params          |
    | partitions                |
    | roles                     |
    | sd_params                 |
    | sds                       |
    | sequence_table            |
    | serde_params              |
    | serdes                    |
    | skewed_col_names          |
    | skewed_col_value_loc_map  |
    | skewed_string_list        |
    | skewed_string_list_values |
    | skewed_values             |
    | sort_cols                 |
    | tab_col_stats             |
    | table_params              |
    | tbl_col_privs             |
    | tbl_privs                 |
    | tbls                      |
    | version                   |
    +---------------------------+
    35 rows in set (0.00 sec)
	
 **注意：dbs,tbls,cds,sds,partitions** 
 
 在MySQL中查看dbs,dbs是hive中的数据库的元数据信息
 
    mysql> select * from dbs\G;
    *************************** 1. row ***************************
              DB_ID: 1
               DESC: Default Hive database
    DB_LOCATION_URI: hdfs://localhost:9000/user/hive/warehouse
               NAME: default
         OWNER_NAME: public
         OWNER_TYPE: ROLE
    *************************** 2. row ***************************
              DB_ID: 6
               DESC: NULL
    DB_LOCATION_URI: hdfs://localhost:9000/user/hive/warehouse/hive.db
               NAME: hive
         OWNER_NAME: hadoop
         OWNER_TYPE: USER
    *************************** 3. row ***************************
              DB_ID: 7
               DESC: NULL
    DB_LOCATION_URI: hdfs://localhost:9000/d6_hive/test
               NAME: hive2
         OWNER_NAME: hadoop
         OWNER_TYPE: USER
    *************************** 4. row ***************************
              DB_ID: 11
               DESC: NULL
    DB_LOCATION_URI: hdfs://localhost:9000/user/hive/warehouse/testdb.db
               NAME: testdb
         OWNER_NAME: hadoop
         OWNER_TYPE: USER
    *************************** 5. row ***************************
              DB_ID: 12
               DESC: NULL
    DB_LOCATION_URI: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db
               NAME: d6_hive
         OWNER_NAME: hadoop
         OWNER_TYPE: USER
    *************************** 6. row ***************************
              DB_ID: 16
               DESC: NULL
    DB_LOCATION_URI: hdfs://localhost:9000/user/hive/warehouse/g6_hadoop.db
               NAME: g6_hadoop
         OWNER_NAME: hadoop
         OWNER_TYPE: USER
    6 rows in set (0.00 sec)
    
    ERROR: 
    No query specified

**在mysql中查看sds**

    mysql> select * from sds\G;
    *************************** 1. row ***************************
                        SD_ID: 1
                        CD_ID: 1
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_wc
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 1
    *************************** 2. row ***************************
                        SD_ID: 6
                        CD_ID: 6
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db/emp
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 6
    *************************** 3. row ***************************
                        SD_ID: 11
                        CD_ID: 11
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db/emp2
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 11
    *************************** 4. row ***************************
                        SD_ID: 16
                        CD_ID: 16
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db/dept
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 16
    *************************** 5. row ***************************
                        SD_ID: 21
                        CD_ID: 21
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db/dual
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 21
    *************************** 6. row ***************************
                        SD_ID: 32
                        CD_ID: 32
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db/hive_wc
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 32
    *************************** 7. row ***************************
                        SD_ID: 36
                        CD_ID: 36
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/col_to_row_test
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 36
    *************************** 8. row ***************************
                        SD_ID: 46
                        CD_ID: 46
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db/order_partition
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 46
    *************************** 9. row ***************************
                        SD_ID: 47
                        CD_ID: 46
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db/order_partition/event_month=2014-05
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 47
    *************************** 10. row ***************************
                        SD_ID: 48
                        CD_ID: 46
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db/order_partition/event_month=2014-06
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 48
    *************************** 11. row ***************************
                        SD_ID: 49
                        CD_ID: 47
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db/order_mulit_partition
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 49
    *************************** 12. row ***************************
                        SD_ID: 50
                        CD_ID: 47
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db/order_mulit_partition/event_month=2014-05/step=1
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 50
    *************************** 13. row ***************************
                        SD_ID: 51
                        CD_ID: 51
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db/emp_static_partition
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 51
    *************************** 14. row ***************************
                        SD_ID: 52
                        CD_ID: 51
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db/emp_static_partition/deptno=10
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 52
    *************************** 15. row ***************************
                        SD_ID: 54
                        CD_ID: 53
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db/emp_dynamic_partition
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 54
    *************************** 16. row ***************************
                        SD_ID: 55
                        CD_ID: 53
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db/emp_dynamic_partition/deptno=30
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 55
    *************************** 17. row ***************************
                        SD_ID: 56
                        CD_ID: 53
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db/emp_dynamic_partition/deptno=10
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 56
    *************************** 18. row ***************************
                        SD_ID: 57
                        CD_ID: 53
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db/emp_dynamic_partition/deptno=__HIVE_DEFAULT_PARTITION__
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 57
    *************************** 19. row ***************************
                        SD_ID: 58
                        CD_ID: 53
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db/emp_dynamic_partition/deptno=20
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 58
    *************************** 20. row ***************************
                        SD_ID: 63
                        CD_ID: 58
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db/hive_array
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 63
    *************************** 21. row ***************************
                        SD_ID: 64
                        CD_ID: 59
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db/hive_map
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 64
    *************************** 22. row ***************************
                        SD_ID: 65
                        CD_ID: 60
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db/hive_struct
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 65
    *************************** 23. row ***************************
                        SD_ID: 67
                        CD_ID: 62
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db/hive_union
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 67
    *************************** 24. row ***************************
                        SD_ID: 68
                        CD_ID: 63
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db/union_test3
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 68
    *************************** 25. row ***************************
                        SD_ID: 70
                        CD_ID: 65
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/user/hive/warehouse/d6_hive.db/union_testnew
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 70
    *************************** 26. row ***************************
                        SD_ID: 71
                        CD_ID: 66
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/g6/hadoop/access/clear
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 71
    *************************** 27. row ***************************
                        SD_ID: 76
                        CD_ID: 66
                 INPUT_FORMAT: org.apache.hadoop.mapred.TextInputFormat
                IS_COMPRESSED:  
    IS_STOREDASSUBDIRECTORIES:  
                     LOCATION: hdfs://localhost:9000/g6/hadoop/access/clear/day=20190328
                  NUM_BUCKETS: -1
                OUTPUT_FORMAT: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                     SERDE_ID: 76
    27 rows in set (0.01 sec)

**在MySQL中查看tbls**

    mysql> select * from tbls\G;
    *************************** 1. row ***************************
                TBL_ID: 1
           CREATE_TIME: 1552944715
                 DB_ID: 1
      LAST_ACCESS_TIME: 0
                 OWNER: hadoop
             RETENTION: 0
                 SD_ID: 1
              TBL_NAME: d6_wc
              TBL_TYPE: MANAGED_TABLE
    VIEW_EXPANDED_TEXT: NULL
    VIEW_ORIGINAL_TEXT: NULL
    *************************** 2. row ***************************
                TBL_ID: 6
           CREATE_TIME: 1552961328
                 DB_ID: 12
      LAST_ACCESS_TIME: 0
                 OWNER: hadoop
             RETENTION: 0
                 SD_ID: 6
              TBL_NAME: emp
              TBL_TYPE: MANAGED_TABLE
    VIEW_EXPANDED_TEXT: NULL
    VIEW_ORIGINAL_TEXT: NULL
    
 **在MySQL中查看分区信息**
    mysql> select * from partitions\G;
    *************************** 1. row ***************************
             PART_ID: 6
         CREATE_TIME: 1553373298
    LAST_ACCESS_TIME: 0
           PART_NAME: event_month=2014-05
               SD_ID: 47
              TBL_ID: 46
    *************************** 2. row ***************************
             PART_ID: 7
         CREATE_TIME: 1553373768
    LAST_ACCESS_TIME: 0
           PART_NAME: event_month=2014-06
               SD_ID: 48
              TBL_ID: 46
    *************************** 3. row ***************************
             PART_ID: 8
         CREATE_TIME: 1553374636
    LAST_ACCESS_TIME: 0
           PART_NAME: event_month=2014-05/step=1
               SD_ID: 50
              TBL_ID: 47
    *************************** 4. row ***************************
             PART_ID: 11
         CREATE_TIME: 1553381184
    LAST_ACCESS_TIME: 0
           PART_NAME: deptno=10
               SD_ID: 52
              TBL_ID: 51
    *************************** 5. row ***************************
             PART_ID: 12
         CREATE_TIME: 1553381778
    LAST_ACCESS_TIME: 0
           PART_NAME: deptno=30
               SD_ID: 55
              TBL_ID: 53
    *************************** 6. row ***************************
             PART_ID: 13
         CREATE_TIME: 1553381778
    LAST_ACCESS_TIME: 0
           PART_NAME: deptno=10
               SD_ID: 56
              TBL_ID: 53
    *************************** 7. row ***************************
             PART_ID: 14
         CREATE_TIME: 1553381778
    LAST_ACCESS_TIME: 0
           PART_NAME: deptno=__HIVE_DEFAULT_PARTITION__
               SD_ID: 57
              TBL_ID: 53
    *************************** 8. row ***************************
             PART_ID: 15
         CREATE_TIME: 1553381778
    LAST_ACCESS_TIME: 0
           PART_NAME: deptno=20
               SD_ID: 58
              TBL_ID: 53
    *************************** 9. row ***************************
             PART_ID: 16
         CREATE_TIME: 1553523756
    LAST_ACCESS_TIME: 0
           PART_NAME: day=20190328
               SD_ID: 76
              TBL_ID: 66
    9 rows in set (0.00 sec)
    
**注意：Hive中的数据由2部分组成，存储在HDFS上的数据和MySQL中的元数据，二者缺一不可。**

### 思考题
请把hiveserver2给我启动在16666端口上，并使用beeline访问通 (required)
答案：在hive-sit.xml 文件增加

    <property>
       <name>hive.server2.thrift.port</name>
       <value>16666</value>                              
    </property>

